includes:
- common/defaults/configs/datasets/vqa/m4c_textvqa_ocr100.yml
# Use soft copy
dataset_attributes:
  m4c_textvqa:
    image_features:
      train:
      - feat_resx/train,ocr_feat_resx/textvqa_conf/train_images
      val:
      - feat_resx/train,ocr_feat_resx/textvqa_conf/train_images
      test:
      - feat_resx/test,ocr_feat_resx/textvqa_conf/test_images
    imdb_files:
      train:
      - imdb/m4c_textvqa/imdb_train_ocr_en.npy
      val:
      - imdb/m4c_textvqa/imdb_val_ocr_en.npy
      test:
      - imdb/m4c_textvqa/imdb_test_ocr_en.npy
    processors:
      text_processor:
        type: bert_tokenizer
        params:
          max_length: 20
      context_processor:
        params:
          max_length: 200
      answer_processor:
        type: m4c_answer
        params:
          vocab_file: m4c_vocabs/textvqa/fixed_answer_vocab_textvqa_5k.txt
          preprocessor:
            type: simple_word
            params: {}
          context_preprocessor:
            type: simple_word
            params: {}
          max_length: 200
          max_copy_steps: 12
          num_answers: 10
      copy_processor:
        type: copy
        params:
          obj_max_length: 100
          max_length: 100
      phoc_processor:
        type: phoc
        params:
          max_length: 200
    ocr:
      features: 'label+bert+char+cons' #label+bert+char+cons
      adv_pro: 0.35
      label_list: [0.9,0.9]
model_attributes:
  twa:
    lr_scale_frcn: 0.1
    lr_scale_text_bert: 0.1
    lr_scale_mmt: 1.0  # no scaling
    text_bert_init_from_bert_base: true
    text_bert:
      num_hidden_layers: 3
    vocab:
      char_max_num: 50
      features: ''
      use_char_decoding: false
      char_decoding_max_step: 70
      char_num: 96
      fusion: sa
      bias: true
    obj:
      mmt_in_dim: 2048
      dropout_prob: 0.1
    ocr:
      mmt_in_dim: 3152 #3052  # 300 (FastText) + 604 (PHOC) + 2048 (Faster R-CNN) + 100 (all zeros; legacy)
      dropout_prob: 0.1
      features: 'char+bert' #char bert
      con_mnt_rel: true
      use_ocr_conf: true
    mmt:
      hidden_size: 768
      num_hidden_layers: 4
    classifier:
      type: linear
      ocr_max_num: 200
      ocr_ptr_net:
        hidden_size: 768
        query_key_size: 768
      params: {}
    model_data_dir: ../data
    metrics:
    - type: maskpred_accuracy
    losses:
    - type: pretrainonly_m4c_decoding_bce_with_mask
optimizer_attributes:
  params:
    eps: 1.0e-08
    lr: 1e-4
    weight_decay: 0
  type: Adam
training_parameters:
    clip_norm_mode: all
    clip_gradients: true
    max_grad_l2_norm: 0.25
    lr_scheduler: true
    lr_steps:
    - 21000
    - 29000
    lr_ratio: 0.1
    use_warmup: true
    warmup_factor: 0.2
    warmup_iterations: 1000
    max_iterations: 36000
    batch_size: 48
    num_workers: 8
    task_size_proportional_sampling: true
    monitored_metric: m4c_textvqa/maskpred_accuracy
    metric_minimize: false
